{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5c07a64",
   "metadata": {},
   "source": [
    "# A Human-Interpretable Multi-Document Reasoning System Powered by LLaMA\n",
    "\n",
    "This notebook provides a runnable, end-to-end scaffold for the project: ingestion, preprocessing, dense retrieval (FAISS), basic NER, KG construction stub, RAG inference pipeline, and LoRA/QLoRA fine-tuning recipes. **Note:** model weights are not included. Install and run on a machine with appropriate GPU(s).\n",
    "\n",
    "**How to use:** Run cells sequentially. Several cells include `!pip install` commands to install required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0618fcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: install required packages (run this cell first)\n",
    "# You may need to restart the kernel after some installs.\n",
    "!pip install --upgrade pip\n",
    "!pip install transformers==4.34.0 sentence-transformers faiss-cpu datasets[parquet] nltk spacy pyvis streamlit==1.24.1 neo4j pandas scikit-learn tqdm jupyterlab\n",
    "# For LoRA / QLoRA & bitsandbytes (requires CUDA & compatible drivers) - uncomment if GPU available\n",
    "# !pip install peft accelerate bitsandbytes\n",
    "# Install a spaCy model\n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd449533",
   "metadata": {},
   "source": [
    "## Sample documents\n",
    "Below we create a tiny set of sample documents to run the pipeline end-to-end. Replace this with your PDFs / scraped pages in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f33e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small sample corpus\n",
    "docs = [\n",
    "    { 'id': 'doc1', 'text': \"Apple acquired Beats in 2014. The company continued expanding into audio products.\" },\n",
    "    { 'id': 'doc2', 'text': \"In 2014, Apple bought Beats Electronics. Tim Cook announced the acquisition.\" },\n",
    "    { 'id': 'doc3', 'text': \"Tesla delivered 1.2M cars in 2023 according to their report. Another source claims 1.4M deliveries.\" },\n",
    "    { 'id': 'doc4', 'text': \"Beats was founded by Dr. Dre and Jimmy Iovine before being acquired by Apple in 2014.\"}\n",
    "]\n",
    "import json, os\n",
    "os.makedirs('/mnt/data/project_assets', exist_ok=True)\n",
    "with open('/mnt/data/project_assets/sample_docs.json', 'w') as f:\n",
    "    json.dump(docs, f, indent=2)\n",
    "print('Sample documents saved to /mnt/data/project_assets/sample_docs.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d069033f",
   "metadata": {},
   "source": [
    "## Preprocessing & Chunking\n",
    "Chunk documents into passages for retrieval. Adjust chunk size for your use-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c688eb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "with open('/mnt/data/project_assets/sample_docs.json') as f:\n",
    "    docs = json.load(f)\n",
    "\n",
    "# simple sentence-based chunking\n",
    "passages = []\n",
    "for d in docs:\n",
    "    sents = sent_tokenize(d['text'])\n",
    "    for i, s in enumerate(sents):\n",
    "        passages.append({\n",
    "            'doc_id': d['id'],\n",
    "            'passage_id': f\"{d['id']}_s{i}\",\n",
    "            'text': s\n",
    "        })\n",
    "\n",
    "import pandas as pd\n",
    "df_passages = pd.DataFrame(passages)\n",
    "df_passages.to_csv('/mnt/data/project_assets/passages.csv', index=False)\n",
    "df_passages.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f76f031",
   "metadata": {},
   "source": [
    "## Build embeddings and FAISS index\n",
    "Uses `sentence-transformers` to create embeddings and FAISS to index them. If you have a GPU, use a GPU-enabled model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501efdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Load a small SBERT model (CPU compatible). On GPU use a larger model.\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # ~100MB, good for demos\n",
    "texts = df_passages['text'].tolist()\n",
    "embs = model.encode(texts, show_progress_bar=True, convert_to_numpy=True)\n",
    "\n",
    "# Build FAISS index\n",
    "d = embs.shape[1]\n",
    "index = faiss.IndexFlatIP(d)  # inner product (use normalized vectors for cosine)\n",
    "# normalize vectors for cosine similarity\n",
    "faiss.normalize_L2(embs)\n",
    "index.add(embs)\n",
    "print('FAISS index built with', index.ntotal, 'vectors')\n",
    "\n",
    "# save index and metadata\n",
    "faiss.write_index(index, '/mnt/data/project_assets/faiss_index.idx')\n",
    "with open('/mnt/data/project_assets/passages_meta.pkl','wb') as f:\n",
    "    pickle.dump(df_passages, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2444f0df",
   "metadata": {},
   "source": [
    "## Retrieval demo\n",
    "Retrieve top-k passages for a query and show their provenance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f3d4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, k=3):\n",
    "    q_emb = model.encode([query], convert_to_numpy=True)\n",
    "    faiss.normalize_L2(q_emb)\n",
    "    D, I = index.search(q_emb, k)\n",
    "    results = []\n",
    "    for score, idx in zip(D[0], I[0]):\n",
    "        results.append((float(score), df_passages.iloc[int(idx)].to_dict()))\n",
    "    return results\n",
    "\n",
    "query = \"When did Apple acquire Beats?\"\n",
    "results = retrieve(query, k=5)\n",
    "for score, meta in results:\n",
    "    print(f\"score={score:.3f}\\t doc={meta['doc_id']}\\t passage={meta['passage_id']}\\n  {meta['text']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c67823",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (SpaCy)\n",
    "Run a simple NER pass to extract entities from passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56b3aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "for _, row in df_passages.iterrows():\n",
    "    print(row['passage_id'], extract_entities(row['text']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5fa396",
   "metadata": {},
   "source": [
    "## Relation Extraction (placeholder)\n",
    "Relation extraction is often task-specific. Below is a simple heuristic extractor for 'acquire' relations. Replace with a trained RE model (REBEL/T5) for production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae99de71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very simple heuristic relation extractor\n",
    "def extract_acquisition_relations(text):\n",
    "    text_lower = text.lower()\n",
    "    if 'acquir' in text_lower or 'bought' in text_lower or 'acquired' in text_lower:\n",
    "        # naive entity extraction around keywords using spaCy\n",
    "        doc = nlp(text)\n",
    "        companies = [ent.text for ent in doc.ents if ent.label_ in ('ORG','PERSON')]\n",
    "        return companies\n",
    "    return []\n",
    "\n",
    "for _, row in df_passages.iterrows():\n",
    "    rels = extract_acquisition_relations(row['text'])\n",
    "    if rels:\n",
    "        print(row['passage_id'], '->', rels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2763ad3",
   "metadata": {},
   "source": [
    "## Knowledge Graph (Neo4j) â€” stub\n",
    "This cell shows how to prepare triples and (optionally) push them to Neo4j. Here we build a simple in-memory triple list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe72b7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build triples from heuristic RE\n",
    "triples = []\n",
    "for _, row in df_passages.iterrows():\n",
    "    rels = extract_acquisition_relations(row['text'])\n",
    "    if rels:\n",
    "        # if sentence contains acquisition, assume first org is acquirer, second is acquired (very naive)\n",
    "        if len(rels) >= 2:\n",
    "            triples.append((rels[0], 'acquired', rels[1], row['doc_id'], row['passage_id']))\n",
    "        elif len(rels) == 1:\n",
    "            triples.append((rels[0], 'mentioned_in', row['doc_id'], row['doc_id'], row['passage_id']))\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame(triples, columns=['head','relation','tail','src_doc','passage_id']).to_csv('/mnt/data/project_assets/triples.csv', index=False)\n",
    "print('Triples saved to /mnt/data/project_assets/triples.csv')\n",
    "pd.read_csv('/mnt/data/project_assets/triples.csv').head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536304c1",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation (RAG) - inference recipe\n",
    "Below is an example of how you would wire retrieval results into an LLM prompt and generate an answer. This cell uses a placeholder HF model; replace with your LLaMA checkpoint and LoRA adapter when available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d264a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "# Small CPU-friendly model for demo - replace with LLaMA (or a LLaMA-derivative) when running on GPU with proper weights.\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
    "model_lm = AutoModelForCausalLM.from_pretrained('distilgpt2')\n",
    "\n",
    "def build_rag_prompt(question, retrieved):\n",
    "    ctx = '\\n'.join([f\"[{r['doc_id']}|{r['passage_id']}] {r['text']}\" for _, r in retrieved])\n",
    "    prompt = f\"Context:\\n{ctx}\\n\\nQuestion: {question}\\nAnswer concisely and cite passages in brackets like [docID|passageID].\\n\"\n",
    "    return prompt\n",
    "\n",
    "question = \"When did Apple acquire Beats and who founded Beats?\"\n",
    "prompt = build_rag_prompt(question, results)\n",
    "print(prompt)\n",
    "# Generate (short)\n",
    "inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=512)\n",
    "out = model_lm.generate(**inputs, max_new_tokens=100)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e95dd12",
   "metadata": {},
   "source": [
    "## LoRA / QLoRA fine-tuning recipe (instructional)\n",
    "Below is a template (non-executable in CPU-only environments) showing how to fine-tune a causal LLaMA-family model with LoRA using `peft`. Replace model paths and dataset with your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad924788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example LoRA fine-tuning template (requires GPU and proper drivers)\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# from datasets import load_dataset\n",
    "# from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "# import bitsandbytes as bnb\n",
    "#\n",
    "# model_name = 'meta-llama/Llama-2-7b'  # example\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit=True, device_map='auto')\n",
    "#\n",
    "# # Prepare for k-bit training\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "# lora_config = LoraConfig(r=8, lora_alpha=32, target_modules=['q_proj','v_proj'], lora_dropout=0.05, bias='none', task_type='CAUSAL_LM')\n",
    "# model = get_peft_model(model, lora_config)\n",
    "#\n",
    "# # Load your dataset formatted as {\"input\": \"...\", \"output\": \"...\"}\n",
    "# dataset = load_dataset('json', data_files={'train':'train.json','validation':'val.json'})\n",
    "#\n",
    "# # Use Trainer/Accelerate to run training. See peft docs for examples.\n",
    "# print('See template - run on GPU-enabled machine with bitsandbytes, peft, accelerate installed.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6e6f08",
   "metadata": {},
   "source": [
    "## Evaluation & Next Steps\n",
    "Suggested metrics and steps to expand the notebook into a full project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b6ad22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Suggested evaluations:\\n- QA: Exact Match (EM) / F1\\n- Citation fidelity: % of claims grounded in cited passages\\n- Verification accuracy on FEVER-style data\\n- Calibration (Brier score) for confidence outputs\\n\\nNext steps:\\n- Replace heuristic RE with a trained RE model (ReBEL/T5)\\n- Integrate a LLaMA LoRA adapter for answer generation\\n- Add a Streamlit demo and Neo4j-backed KG visualizer\\n- Prepare ablation experiments and run on larger datasets (HotpotQA, FEVER)\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
